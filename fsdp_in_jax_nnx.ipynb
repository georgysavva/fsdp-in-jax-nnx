{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPTzwfAgYWGi"
      },
      "source": [
        "# Diffusion Models from Scratch\n",
        "\n",
        "Sometimes it is helpful to consider the simplest possible version of something to better understand how it works. We're going to try that in this notebook, beginning with a 'toy' diffusion model to see how the different pieces work, and then examining how they differ from a more complex implementation.\n",
        "\n",
        "We will look at\n",
        "- The corruption process (adding noise to data)\n",
        "- What a UNet is, and how to implement an extremely minimal one from scratch\n",
        "- Diffusion model training\n",
        "- Sampling theory\n",
        "\n",
        "Then we'll compare our versions with the diffusers DDPM implementation, exploring\n",
        "- Improvements over our mini UNet\n",
        "- The DDPM noise schedule\n",
        "- Differences in training objective\n",
        "- Timestep conditioning\n",
        "- Sampling approaches\n",
        "\n",
        "This notebook is fairly in-depth, and can safely be skipped if you're not excited about a from-scratch deep dive!\n",
        "\n",
        "It is also worth noting that most of the code here is for illustrative purposes, and I wouldn't recommend directly adopting any of it for your own work (unless you're just trying improve on the examples shown here for learning purposes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QHaxHmpjSl7E"
      },
      "outputs": [],
      "source": [
        "GPU=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mr5W7P-Ybhq"
      },
      "source": [
        "## Setup and Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxnsqe0MQ0m4",
        "outputId": "2ca3fb1c-a341-4616-fa1a-d85e5b2c8c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing jax[tpu]==0.5.1 ...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "package = \"jax[cuda12]==0.5.1\" if GPU else \"jax[tpu]==0.5.1\"\n",
        "print(f\"Installing {package} ...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cYwmzUaOSl7E",
        "outputId": "485e525c-1980-4e8c-ce18-1068587b6548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optax==0.2.4 in /usr/local/lib/python3.12/dist-packages (0.2.4)\n",
            "Requirement already satisfied: orbax-checkpoint==0.11.16 in /usr/local/lib/python3.12/dist-packages (0.11.16)\n",
            "Requirement already satisfied: flax==0.10.4 in /usr/local/lib/python3.12/dist-packages (0.10.4)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax==0.2.4) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax==0.2.4) (0.1.90)\n",
            "Requirement already satisfied: jax>=0.4.27 in /usr/local/lib/python3.12/dist-packages (from optax==0.2.4) (0.5.1)\n",
            "Requirement already satisfied: jaxlib>=0.4.27 in /usr/local/lib/python3.12/dist-packages (from optax==0.2.4) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from optax==0.2.4) (1.26.4)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.12/dist-packages (from optax==0.2.4) (1.13.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint==0.11.16) (4.15.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint==0.11.16) (1.1.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint==0.11.16) (6.0.2)\n",
            "Requirement already satisfied: tensorstore>=0.1.71 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint==0.11.16) (0.1.76)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint==0.11.16) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint==0.11.16) (6.32.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint==0.11.16) (4.13.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint==0.11.16) (3.20.1)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax==0.10.4) (14.1.0)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax==0.10.4) (0.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax==0.2.4) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax==0.2.4) (1.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.27->optax==0.2.4) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.27->optax==0.2.4) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.27->optax==0.2.4) (1.16.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax==0.10.4) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax==0.10.4) (2.19.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.11.16) (2025.9.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.11.16) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.11.16) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.10.4) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install optax==0.2.4 orbax-checkpoint==0.11.16 flax==0.10.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xSMLegmvLLJe"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import functools\n",
        "import logging\n",
        "import os\n",
        "from typing import Any, Generator, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "import torch\n",
        "from flax import nnx\n",
        "from jax import random\n",
        "from jax.experimental import mesh_utils\n",
        "from matplotlib.figure import Figure\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-b6V44MUSl7E",
        "outputId": "219f68ed-9519-4687-93ce-1d5151eec148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:2025-09-20 22:51:22,447:jax._src.distributed:125: JAX detected proxy variable(s) in the environment as distributed setup: MODEL_PROXY_HOST COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT COLAB_KERNEL_MANAGER_PROXY_HOST COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS COLAB_KERNEL_MANAGER_PROXY_PORT COLAB_LANGUAGE_SERVER_PROXY. On some systems, this may cause a hang of distributed.initialize and you may need to unset these ENV variable(s)\n",
            "WARNING:jax._src.distributed:JAX detected proxy variable(s) in the environment as distributed setup: MODEL_PROXY_HOST COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT COLAB_KERNEL_MANAGER_PROXY_HOST COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS COLAB_KERNEL_MANAGER_PROXY_PORT COLAB_LANGUAGE_SERVER_PROXY. On some systems, this may cause a hang of distributed.initialize and you may need to unset these ENV variable(s)\n"
          ]
        }
      ],
      "source": [
        "if not GPU:\n",
        "    jax.distributed.initialize()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X5E-eeJQSl7E",
        "outputId": "cfe27856-fc0b-4ef6-81f0-57906b1c103f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "jax.devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_U5MtEG0Sl7F"
      },
      "outputs": [],
      "source": [
        "def setup_logging() -> None:\n",
        "    \"\"\"Setup logging configuration for INFO level console output.\"\"\"\n",
        "    # Configure logging format\n",
        "    log_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
        "\n",
        "    # Setup basic logging configuration\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=log_format,\n",
        "        handlers=[logging.StreamHandler()],  # Console output only\n",
        "        force=True,  # Override any existing configuration\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jqto_JiYSl7F"
      },
      "outputs": [],
      "source": [
        "class SinDataset(Dataset):\n",
        "    \"\"\"A PyTorch dataset that generates sine function data points.\n",
        "\n",
        "    This dataset generates random x values from [-π, π] and computes y = sin(x).\n",
        "    The dataset uses a seeded random number generator for reproducible results.\n",
        "\n",
        "    Args:\n",
        "        seed: Random seed for reproducible data generation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, seed: int) -> None:\n",
        "        \"\"\"Initialize the dataset with a random seed.\n",
        "\n",
        "        Args:\n",
        "            seed: Random seed for data generation.\n",
        "        \"\"\"\n",
        "        self.seed = seed\n",
        "        self.reset_seed()\n",
        "\n",
        "    def reset_seed(self) -> None:\n",
        "        \"\"\"Reset the random number generator to the initial seed.\n",
        "\n",
        "        This is useful for ensuring reproducible evaluation data.\n",
        "        \"\"\"\n",
        "        self.rng = torch.Generator()\n",
        "        self.rng.manual_seed(self.seed)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            A very large number representing the dataset size.\n",
        "        \"\"\"\n",
        "        return 2**31 - 1\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Generate a single data point.\n",
        "\n",
        "        Args:\n",
        "            idx: Index (unused, but required for Dataset interface).\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (x, y) where x is a random value in [-π, π] and y = sin(x).\n",
        "        \"\"\"\n",
        "        x = torch.rand(1, generator=self.rng) * 2 * torch.pi - torch.pi\n",
        "        y = torch.sin(x)\n",
        "        return x.numpy(), y.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o1Lj5lHGSl7F"
      },
      "outputs": [],
      "source": [
        "\n",
        "IN_FEATURES = 1\n",
        "OUT_FEATURES = 1\n",
        "HIDDEN_DIM = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jEQsFGXcSl7F"
      },
      "outputs": [],
      "source": [
        "class MLP(nnx.Module):\n",
        "    \"\"\"A Multi-Layer Perceptron (MLP) neural network using Flax NNX.\n",
        "\n",
        "    This is a simple feedforward neural network with two hidden layers,\n",
        "    ReLU activations, and dropout regularization.\n",
        "\n",
        "    Args:\n",
        "        din: Number of input features.\n",
        "        dmid: Number of hidden units in each hidden layer.\n",
        "        dout: Number of output features.\n",
        "        rngs: Random number generators for parameter initialization and dropout.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, din: int, dmid: int, dout: int, *, rngs: nnx.Rngs) -> None:\n",
        "        \"\"\"Initialize the MLP with specified dimensions.\n",
        "\n",
        "        Args:\n",
        "            din: Number of input features.\n",
        "            dmid: Number of hidden units in each hidden layer.\n",
        "            dout: Number of output features.\n",
        "            rngs: Random number generators for parameter initialization and dropout.\n",
        "        \"\"\"\n",
        "        self.fc1 = nnx.Linear(din, dmid, rngs=rngs)\n",
        "        self.fc2 = nnx.Linear(dmid, dmid, rngs=rngs)\n",
        "        self.dropout = nnx.Dropout(rate=0.1, rngs=rngs)\n",
        "        self.fc3 = nnx.Linear(dmid, dout, rngs=rngs)\n",
        "        self.rngs = rngs\n",
        "\n",
        "    def __call__(self, x: jax.Array) -> jax.Array:\n",
        "        \"\"\"Forward pass through the MLP.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, din).\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, dout).\n",
        "        \"\"\"\n",
        "        x = self.fc1(x)\n",
        "        x = nnx.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = nnx.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Rpc0UxJ7Sl7F"
      },
      "outputs": [],
      "source": [
        "IN_FEATURES = 1\n",
        "OUT_FEATURES = 1\n",
        "HIDDEN_DIM = 16_384"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m0jNsPY7Sl7F"
      },
      "outputs": [],
      "source": [
        "def init_ema(model: nnx.Module) -> nnx.State:\n",
        "    \"\"\"Initialize exponential moving average (EMA) state for a model.\n",
        "\n",
        "    Creates a zero-initialized state tree with the same structure as the model's state.\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model to create EMA state for.\n",
        "\n",
        "    Returns:\n",
        "        EMA state with the same structure as the model state, but zero-initialized.\n",
        "    \"\"\"\n",
        "    ema_state = jax.tree.map(lambda x: jnp.zeros_like(x), nnx.state(model))\n",
        "    return ema_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "S3gE3Ht0Sl7F"
      },
      "outputs": [],
      "source": [
        "def init(learning_rate: float) -> Tuple[nnx.GraphDef, nnx.State, nnx.State]:\n",
        "    \"\"\"Initialize the model, optimizer, and EMA state.\n",
        "\n",
        "    Creates a new MLP model, wraps it in an AdamW optimizer, and initializes\n",
        "    the exponential moving average state.\n",
        "\n",
        "    Args:\n",
        "        learning_rate: Learning rate for the AdamW optimizer.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (optimizer_graph, optimizer_state, ema_state).\n",
        "    \"\"\"\n",
        "    model = MLP(\n",
        "        IN_FEATURES,\n",
        "        HIDDEN_DIM,\n",
        "        OUT_FEATURES,\n",
        "        rngs=nnx.Rngs(0, dropout=random.key(1), noise=random.key(2)),\n",
        "    )\n",
        "    opt = nnx.Optimizer(\n",
        "        model,\n",
        "        optax.adamw(learning_rate=learning_rate),\n",
        "    )\n",
        "    opt_graph, opt_state = nnx.split(opt)\n",
        "    ema_state = init_ema(model)\n",
        "    return opt_graph, opt_state, ema_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EKdK2Kk4Sl7F"
      },
      "outputs": [],
      "source": [
        "def create_device_mesh(axis_name: str) -> jax.sharding.Mesh:\n",
        "    \"\"\"Create a JAX device mesh for distributed computation.\n",
        "\n",
        "    Creates a 1D mesh using all available devices and assigns the given axis name.\n",
        "\n",
        "    Args:\n",
        "        axis_name: Name to assign to the mesh axis (e.g., 'data' for data parallelism).\n",
        "\n",
        "    Returns:\n",
        "        JAX mesh object for distributed computation.\n",
        "    \"\"\"\n",
        "    device_mesh = mesh_utils.create_device_mesh(\n",
        "        (jax.device_count(),), devices=jax.devices()\n",
        "    )\n",
        "    return jax.sharding.Mesh(device_mesh, (axis_name,))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DU-vVGAASl7F"
      },
      "outputs": [],
      "source": [
        "def build_shardings(\n",
        "    data_axis: str,\n",
        ") -> Tuple[\n",
        "    jax.sharding.Mesh,\n",
        "    jax.sharding.NamedSharding,\n",
        "    jax.sharding.NamedSharding,\n",
        "]:\n",
        "    \"\"\"Build JAX sharding configurations for distributed computation.\n",
        "\n",
        "    Creates a device mesh and two sharding strategies:\n",
        "    - Data sharding: for sharding data across devices\n",
        "    - Replicated sharding: for replicating data across all devices\n",
        "\n",
        "    Args:\n",
        "        data_axis: Name of the axis for data parallelism.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (device_mesh, data_sharding, replicated_sharding).\n",
        "    \"\"\"\n",
        "    device_mesh = create_device_mesh(\n",
        "        data_axis,\n",
        "    )\n",
        "    data_sharding = jax.sharding.NamedSharding(\n",
        "        device_mesh, jax.sharding.PartitionSpec(data_axis)\n",
        "    )\n",
        "    repl_sharding = jax.sharding.NamedSharding(\n",
        "        device_mesh, jax.sharding.PartitionSpec()\n",
        "    )\n",
        "\n",
        "    return device_mesh, data_sharding, repl_sharding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "H1y_d7sjSl7F"
      },
      "outputs": [],
      "source": [
        "def fsdp(\n",
        "    axis: str,\n",
        "    cur_spec: Tuple[Any, ...],\n",
        "    mesh: jax.sharding.Mesh,\n",
        "    var_state: nnx.VariableState,\n",
        "    min_size_to_shard: int,\n",
        ") -> Tuple[Any, ...]:\n",
        "    \"\"\"Implement Fully Sharded Data Parallel (FSDP) sharding strategy.\n",
        "\n",
        "    Determines how to shard a parameter tensor across devices. Shards the largest\n",
        "    dimension that is divisible by the number of devices and meets the minimum size requirement.\n",
        "\n",
        "    Args:\n",
        "        axis: Name of the mesh axis to shard along.\n",
        "        cur_spec: Current partition specification.\n",
        "        mesh: JAX device mesh.\n",
        "        var_state: Variable state containing the parameter tensor.\n",
        "        min_size_to_shard: Minimum tensor size to consider for sharding.\n",
        "\n",
        "    Returns:\n",
        "        Updated partition specification with sharding applied if appropriate.\n",
        "    \"\"\"\n",
        "    arr = var_state.value\n",
        "    if arr is None:\n",
        "        return cur_spec\n",
        "    shape = tuple(arr.shape)\n",
        "    axis_size = mesh.shape[axis]\n",
        "    if arr.size < min_size_to_shard:\n",
        "        return cur_spec\n",
        "    dim_indices = sorted(range(len(shape)), key=lambda i: shape[i], reverse=True)\n",
        "    for i in dim_indices:\n",
        "        if cur_spec[i] is None and shape[i] % axis_size == 0:\n",
        "            new_spec = list(cur_spec)\n",
        "            new_spec[i] = axis\n",
        "            return tuple(new_spec)\n",
        "    return cur_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_P87PEZESl7F"
      },
      "outputs": [],
      "source": [
        "\n",
        "def flatten_state(\n",
        "    state: nnx.State, path: Tuple[str, ...] = ()\n",
        ") -> Generator[Tuple[str, nnx.VariableState], None, None]:\n",
        "    \"\"\"Recursively flatten a nested state tree into (name, variable_state) pairs.\n",
        "\n",
        "    Traverses the state tree and yields each variable with its hierarchical path name.\n",
        "\n",
        "    Args:\n",
        "        state: The state tree to flatten (can be nested).\n",
        "        path: Current path in the hierarchy (used for recursion).\n",
        "\n",
        "    Yields:\n",
        "        Tuples of (path_name, variable_state) for each leaf variable.\n",
        "    \"\"\"\n",
        "    if isinstance(state, nnx.VariableState):\n",
        "        name = \"/\".join(str(p) for p in path)\n",
        "        yield name, state\n",
        "    elif hasattr(state, \"items\"):\n",
        "        for key, subtree in state.items():\n",
        "            yield from flatten_state(subtree, path + (key,))\n",
        "    elif isinstance(state, (list, tuple)):\n",
        "        for idx, subtree in enumerate(state):\n",
        "            yield from flatten_state(subtree, path + (str(idx),))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "r0pYGcuRSl7F"
      },
      "outputs": [],
      "source": [
        "def infer_sharding(\n",
        "    state: nnx.State,\n",
        "    mesh: jax.sharding.Mesh,\n",
        "    axis: str,\n",
        "    min_size_to_shard: int = 2**20,\n",
        ") -> nnx.State:\n",
        "    \"\"\"Infer optimal sharding strategy for a model state using FSDP.\n",
        "\n",
        "    Analyzes each parameter in the state and determines the best sharding strategy\n",
        "    based on tensor size and dimensions. Creates a sharding tree that matches\n",
        "    the structure of the input state.\n",
        "\n",
        "    Args:\n",
        "        state: Model state to create sharding for.\n",
        "        mesh: JAX device mesh for distributed computation.\n",
        "        axis: Name of the mesh axis for sharding.\n",
        "        min_size_to_shard: Minimum tensor size to consider for sharding.\n",
        "\n",
        "    Returns:\n",
        "        Sharding tree with the same structure as the input state.\n",
        "    \"\"\"\n",
        "    flat_params = list(flatten_state(state))\n",
        "    vars_states = [vs for _, vs in flat_params]\n",
        "\n",
        "    specs = [\n",
        "        (None,) * vs.value.ndim if vs.value is not None else () for vs in vars_states\n",
        "    ]\n",
        "\n",
        "    for i, _ in enumerate(flat_params):\n",
        "        specs[i] = fsdp(axis, specs[i], mesh, vars_states[i], min_size_to_shard)\n",
        "\n",
        "    shardings = [\n",
        "        jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(*spec))\n",
        "        for spec in specs\n",
        "    ]\n",
        "\n",
        "    sharding_tree = jax.tree_util.tree_unflatten(\n",
        "        jax.tree_util.tree_structure(\n",
        "            state, is_leaf=lambda x: isinstance(x, nnx.VariableState)\n",
        "        ),\n",
        "        shardings,\n",
        "    )\n",
        "    return sharding_tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Nm-hlk1kSl7F"
      },
      "outputs": [],
      "source": [
        "def log_shard_map(tag: str, state: nnx.State) -> None:\n",
        "    \"\"\"Log the sharding mapping of arrays to devices for debugging.\n",
        "\n",
        "    Prints a detailed breakdown of how each parameter is sharded across devices,\n",
        "    showing which array indices are stored on which devices.\n",
        "\n",
        "    Args:\n",
        "        tag: Descriptive tag for the logging output.\n",
        "        state: Model state to analyze for sharding information.\n",
        "    \"\"\"\n",
        "    logging.info(f\"── Shard ↦ device map: {tag} ──\")\n",
        "\n",
        "    for name, var in flatten_state(state):\n",
        "        arr = var.value if isinstance(var, nnx.VariableState) else var\n",
        "        for d, idx in arr.sharding.devices_indices_map(arr.shape).items():\n",
        "            logging.info(f\" {name}  {idx}  → {d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TXpo8OiXSl7F"
      },
      "outputs": [],
      "source": [
        "def make_fsarray_from_local_slice(\n",
        "    local_slice: jnp.ndarray,\n",
        "    global_devices: list[jax.Device],\n",
        "    axis: str,\n",
        ") -> jax.Array:\n",
        "    \"\"\"Create a globally sharded array from a local data slice.\n",
        "\n",
        "    Takes a local data slice and creates a globally sharded JAX array\n",
        "    by distributing the data across multiple devices and processes.\n",
        "\n",
        "    This function is adapted from:\n",
        "    https://github.com/google-research/big_vision/blob/0127fb6b337ee2a27bf4e54dea79cff176527356/big_vision/utils.py#L1388-L1409\n",
        "\n",
        "    Args:\n",
        "        local_slice: Local portion of the data on this process.\n",
        "        global_devices: List of all devices across all processes.\n",
        "        axis: Name of the axis for sharding.\n",
        "\n",
        "    Returns:\n",
        "        Globally sharded JAX array with proper device placement.\n",
        "    \"\"\"\n",
        "    mesh = jax.sharding.Mesh(global_devices, (axis,))\n",
        "    sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(axis))\n",
        "    local_ds = mesh.local_devices\n",
        "\n",
        "    x = np.asarray(local_slice)\n",
        "    xs = jax.device_put(np.split(x, len(local_ds), axis=0), local_ds)\n",
        "\n",
        "    global_shape = (x.shape[0] * jax.process_count(), *x.shape[1:])\n",
        "    return jax.make_array_from_single_device_arrays(global_shape, sharding, xs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mnngVe5uSl7F"
      },
      "outputs": [],
      "source": [
        "def update_ema(\n",
        "    model_state: nnx.State,\n",
        "    ema_state: nnx.State,\n",
        "    ema_decay: float,\n",
        ") -> nnx.State:\n",
        "    \"\"\"Update exponential moving average (EMA) of model parameters.\n",
        "\n",
        "    Computes the exponential moving average using the formula:\n",
        "    ema_new = ema_decay * ema_old + (1 - ema_decay) * model_param\n",
        "\n",
        "    Args:\n",
        "        model_state: Current model state with updated parameters.\n",
        "        ema_state: Current EMA state to be updated.\n",
        "        ema_decay: Decay factor for EMA (typically close to 1.0, e.g., 0.9999).\n",
        "\n",
        "    Returns:\n",
        "        Updated EMA state.\n",
        "    \"\"\"\n",
        "\n",
        "    def update_param(p_model: jax.Array, p_ema: jax.Array) -> jax.Array:\n",
        "        return p_ema * ema_decay + p_model * (1 - ema_decay)\n",
        "\n",
        "    ema_state_no_rng = jax.tree.map(\n",
        "        update_param,\n",
        "        nnx.filter_state(model_state, nnx.Param),\n",
        "        nnx.filter_state(ema_state, nnx.Param),\n",
        "    )\n",
        "    ema_state = nnx.merge_state(ema_state, ema_state_no_rng)\n",
        "    return ema_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "H3Lj0tHtSl7F"
      },
      "outputs": [],
      "source": [
        "def test_step(\n",
        "    model_graph: nnx.GraphDef,\n",
        "    model_state: nnx.State,\n",
        "    x: jax.Array,\n",
        "    y: jax.Array,\n",
        ") -> Tuple[jax.Array, jax.Array]:\n",
        "    \"\"\"Perform a single evaluation step without parameter updates.\n",
        "\n",
        "    Computes the forward pass and loss for evaluation purposes.\n",
        "\n",
        "    Args:\n",
        "        model_graph: Model graph definition (static structure).\n",
        "        model_state: Model state (parameters only, no optimizer state).\n",
        "        x: Input batch of shape (batch_size, input_dim).\n",
        "        y: Target batch of shape (batch_size, output_dim).\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (loss_value, predictions).\n",
        "    \"\"\"\n",
        "    model = nnx.merge(model_graph, model_state)\n",
        "    y_hat = model(x)\n",
        "    loss = jnp.mean((y_hat - y) ** 2)\n",
        "    return loss, y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OiGZWQrISl7G"
      },
      "outputs": [],
      "source": [
        "def main(args: argparse.Namespace) -> None:\n",
        "    \"\"\"Main training loop for distributed MLP training with FSDP.\n",
        "\n",
        "    Implements a complete training pipeline including:\n",
        "    - Distributed initialization and device mesh setup\n",
        "    - Model and optimizer initialization with FSDP sharding\n",
        "    - Checkpoint loading and saving\n",
        "    - Training loop with EMA updates\n",
        "    - Periodic evaluation and visualization\n",
        "\n",
        "    Args:\n",
        "        args: Command-line arguments containing hyperparameters and configuration.\n",
        "    \"\"\"\n",
        "    # Setup logging\n",
        "    setup_logging()\n",
        "    logging.info(f\"Starting training with args: {args}\")\n",
        "\n",
        "    if not args.gpu:\n",
        "        # assert args.checkpoint_dir.startswith(\n",
        "        #     \"gs://\"\n",
        "        # ), \"Checkpoint directory must be a GCS path\"\n",
        "        jax.distributed.initialize()\n",
        "    logging.info(f\"Available JAX devices: {jax.devices()}\")\n",
        "\n",
        "    data_axis = \"data\"\n",
        "    mesh, data_sharding, repl_sharding = build_shardings(data_axis=\"data\")\n",
        "    init_fn = functools.partial(init, args.lr)\n",
        "    _, opt_state_shape, ema_state_shape = jax.eval_shape(init_fn)\n",
        "    opt_state_sharding = infer_sharding(opt_state_shape, mesh, data_axis)\n",
        "    ema_state_sharding = infer_sharding(ema_state_shape, mesh, data_axis)\n",
        "\n",
        "    opt_graph, opt_state, ema_state = jax.jit(\n",
        "        init_fn,\n",
        "        out_shardings=(repl_sharding, opt_state_sharding, ema_state_sharding),\n",
        "    )()\n",
        "    if jax.process_index() == 0:\n",
        "        log_shard_map(\"Opt state sharding\", opt_state)\n",
        "        log_shard_map(\"EMA state sharding\", ema_state)\n",
        "    if jax.process_index() == 0:\n",
        "        logging.info(\"Merging optimizer graph and state\")\n",
        "    opt = nnx.merge(opt_graph, opt_state)\n",
        "    opt.model.train()\n",
        "    opt_graph, opt_state = nnx.split(opt)\n",
        "    opt.model.eval()\n",
        "    model_graph_eval, _ = nnx.split(opt.model)\n",
        "    ckpt_mngr = ocp.CheckpointManager(\n",
        "        args.checkpoint_dir,\n",
        "        options=ocp.CheckpointManagerOptions(\n",
        "            save_interval_steps=args.save_interval,\n",
        "            max_to_keep=2,\n",
        "            step_prefix=args.experiment_name,\n",
        "            enable_async_checkpointing=False,\n",
        "        ),\n",
        "    )\n",
        "    if jax.process_index() == 0:\n",
        "        logging.info(\"Checkpoint manager initialized\")\n",
        "\n",
        "    latest_step = None\n",
        "    latest_step = ckpt_mngr.latest_step()\n",
        "    if latest_step is not None:\n",
        "\n",
        "        opt_rngs, opt_state_no_rngs = nnx.filter_state(opt_state, nnx.RngKey, ...)\n",
        "        opt_rng_keys = jax.tree.map(jax.random.key_data, opt_rngs)\n",
        "\n",
        "        ema_rngs, ema_state_no_rngs = nnx.filter_state(ema_state, nnx.RngKey, ...)\n",
        "        ema_rng_keys = jax.tree.map(jax.random.key_data, ema_rngs)\n",
        "        log_shard_map(\"Opt state no rngs sharding before restore\", opt_state_no_rngs)\n",
        "        log_shard_map(\"EMA state no rngs sharding before restore\", ema_state_no_rngs)\n",
        "        log_shard_map(\"Opt rngs sharding before restore\", opt_rng_keys)\n",
        "        log_shard_map(\"EMA rngs sharding before restore\", ema_rng_keys)\n",
        "\n",
        "        state_restored = ckpt_mngr.restore(\n",
        "            latest_step,\n",
        "            args=ocp.args.Composite(\n",
        "                opt_state=ocp.args.StandardRestore(opt_state_no_rngs),\n",
        "                ema_state=ocp.args.StandardRestore(ema_state_no_rngs),\n",
        "                opt_rngs=ocp.args.StandardRestore(opt_rng_keys),\n",
        "                ema_rngs=ocp.args.StandardRestore(ema_rng_keys),\n",
        "            ),\n",
        "        )\n",
        "        opt_state_no_rngs, ema_state_no_rngs, opt_rngs_keys, ema_rngs_keys = (\n",
        "            state_restored.opt_state,\n",
        "            state_restored.ema_state,\n",
        "            state_restored.opt_rngs,\n",
        "            state_restored.ema_rngs,\n",
        "        )\n",
        "        opt_rngs = jax.tree_map(jax.random.wrap_key_data, opt_rngs_keys)\n",
        "        ema_rngs = jax.tree_map(jax.random.wrap_key_data, ema_rngs_keys)\n",
        "        if jax.process_index() == 0:\n",
        "            logging.info(\"Checkpoint restored successfully\")\n",
        "            logging.info(f\"Opt state no rngs after restore: {opt_state_no_rngs}\")\n",
        "            logging.info(f\"Opt rngs after restore: {opt_rngs}\")\n",
        "            logging.info(f\"EMA state no rngs after restore: {ema_state_no_rngs}\")\n",
        "            logging.info(f\"EMA rngs after restore: {ema_rngs}\")\n",
        "        opt_state = nnx.merge_state(opt_state_no_rngs, opt_rngs)\n",
        "        ema_state = nnx.merge_state(ema_state_no_rngs, ema_rngs)\n",
        "        if jax.process_index() == 0:\n",
        "            logging.info(\"Checkpoint restored successfully\")\n",
        "            log_shard_map(\"Opt state sharding after restore\", opt_state)\n",
        "            log_shard_map(\"EMA state sharding after restore\", ema_state)\n",
        "            logging.info(f\"Opt state after restore: {opt_state}\")\n",
        "            logging.info(f\"EMA state after restore: {ema_state}\")\n",
        "    start_step = 0 if latest_step is None else latest_step\n",
        "    local_batch_size = args.batch_size // jax.process_count()\n",
        "\n",
        "    if jax.process_index() == 0:\n",
        "        logging.info(f\"Training configuration:\")\n",
        "        logging.info(f\"  - Starting from step: {start_step}\")\n",
        "        logging.info(f\"  - Total processes: {jax.process_count()}\")\n",
        "        logging.info(f\"  - Global batch size: {args.batch_size}\")\n",
        "        logging.info(f\"  - Local batch size: {local_batch_size}\")\n",
        "        logging.info(f\"  - Learning rate: {args.lr}\")\n",
        "        logging.info(f\"  - Steps to run: {args.steps}\")\n",
        "        logging.info(f\"  - Log interval: {args.log_interval}\")\n",
        "        logging.info(f\"  - Test interval: {args.test_interval}\")\n",
        "        logging.info(f\"  - Save interval: {args.save_interval}\")\n",
        "    train_dataloader = DataLoader(\n",
        "        SinDataset(seed=start_step), batch_size=local_batch_size, shuffle=False\n",
        "    )\n",
        "    test_dataset = SinDataset(seed=-1)\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset, batch_size=local_batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    train_step_fn = jax.jit(\n",
        "        train_step,\n",
        "        donate_argnums=(1,),\n",
        "        static_argnums=(4,),\n",
        "        out_shardings=(opt_state_sharding, repl_sharding),\n",
        "    )\n",
        "\n",
        "    test_step_fn = jax.jit(\n",
        "        test_step,\n",
        "        out_shardings=(repl_sharding, data_sharding),\n",
        "    )\n",
        "    update_ema_fn = jax.jit(\n",
        "        update_ema,\n",
        "        out_shardings=ema_state_sharding,\n",
        "        donate_argnums=(1,),\n",
        "    )\n",
        "\n",
        "    train_iter = iter(train_dataloader)\n",
        "    ema_decay = 0.9999\n",
        "\n",
        "    for step in range(start_step, start_step + args.steps):\n",
        "        x_batch, y_batch = next(train_iter)\n",
        "        x_batch = make_fsarray_from_local_slice(\n",
        "            x_batch, mesh.devices.flatten(), data_axis\n",
        "        )\n",
        "        y_batch = make_fsarray_from_local_slice(\n",
        "            y_batch, mesh.devices.flatten(), data_axis\n",
        "        )\n",
        "\n",
        "        opt_state, train_loss = train_step_fn(\n",
        "            opt_graph, opt_state, x_batch, y_batch, args.add_noise\n",
        "        )\n",
        "\n",
        "        ema_state = update_ema_fn(opt_state[\"model\"], ema_state, ema_decay)\n",
        "\n",
        "        if jax.process_index() == 0 and (step + 1) % args.log_interval == 0:\n",
        "            logging.info(f\"Step {step+1}, Train Loss: {train_loss:.6f}\")\n",
        "\n",
        "        if (step + 1) % args.test_interval == 0:\n",
        "            test_dataset.reset_seed()\n",
        "            test_iter = iter(test_dataloader)\n",
        "            x_test, y_test = next(test_iter)\n",
        "            x_test = make_fsarray_from_local_slice(\n",
        "                x_test, mesh.devices.flatten(), data_axis\n",
        "            )\n",
        "            y_test = make_fsarray_from_local_slice(\n",
        "                y_test, mesh.devices.flatten(), data_axis\n",
        "            )\n",
        "            test_loss, y_pred_model = test_step_fn(\n",
        "                model_graph_eval, opt_state[\"model\"], x_test, y_test\n",
        "            )\n",
        "\n",
        "            test_loss_ema, y_pred_ema = test_step_fn(\n",
        "                model_graph_eval, ema_state, x_test, y_test\n",
        "            )\n",
        "\n",
        "            y_pred_model = jax.experimental.multihost_utils.process_allgather(\n",
        "                y_pred_model, tiled=True\n",
        "            )\n",
        "            y_pred_ema = jax.experimental.multihost_utils.process_allgather(\n",
        "                y_pred_ema, tiled=True\n",
        "            )\n",
        "            x_test = jax.experimental.multihost_utils.process_allgather(\n",
        "                x_test, tiled=True\n",
        "            )\n",
        "            y_test = jax.experimental.multihost_utils.process_allgather(\n",
        "                y_test, tiled=True\n",
        "            )\n",
        "\n",
        "            if jax.process_index() == 0:\n",
        "                x_plot = np.array(x_test).flatten()\n",
        "                y_true_plot = np.array(y_test).flatten()\n",
        "                y_pred_ema_plot = np.array(y_pred_ema).flatten()\n",
        "                y_pred_model_plot = np.array(y_pred_model).flatten()\n",
        "\n",
        "                sort_idx = np.argsort(x_plot)\n",
        "                x_plot = x_plot[sort_idx]\n",
        "                y_true_plot = y_true_plot[sort_idx]\n",
        "                y_pred_ema_plot = y_pred_ema_plot[sort_idx]\n",
        "                y_pred_model_plot = y_pred_model_plot[sort_idx]\n",
        "\n",
        "                experiment_output_dir = os.path.join(\n",
        "                    args.output_dir, args.experiment_name\n",
        "                )\n",
        "                os.makedirs(experiment_output_dir, exist_ok=True)\n",
        "                fig = Figure(figsize=(10, 6))\n",
        "                ax = fig.add_subplot(111)\n",
        "                ax.scatter(x_plot, y_true_plot, alpha=0.7, label=\"Ground Truth\", s=20)\n",
        "                ax.scatter(\n",
        "                    x_plot,\n",
        "                    y_pred_model_plot,\n",
        "                    alpha=0.7,\n",
        "                    label=\"Model Prediction\",\n",
        "                    s=20,\n",
        "                )\n",
        "                ax.scatter(\n",
        "                    x_plot,\n",
        "                    y_pred_ema_plot,\n",
        "                    alpha=0.7,\n",
        "                    label=\"EMA Prediction\",\n",
        "                    s=20,\n",
        "                )\n",
        "                ax.set_xlabel(\"X\")\n",
        "                ax.set_ylabel(\"Y\")\n",
        "                ax.set_title(\"Sin Function: Ground Truth vs Model vs EMA Prediction\")\n",
        "                ax.legend()\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "                plot_path = os.path.join(experiment_output_dir, f\"eval_{step+1}.png\")\n",
        "                fig.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
        "\n",
        "                logging.info(f\"Plot saved to {plot_path}\")\n",
        "\n",
        "                if jax.process_index() == 0:\n",
        "                    logging.info(\n",
        "                        f\"Step {step+1}, Test Loss: {test_loss:.6f}, \"\n",
        "                        f\"EMA Test Loss: {test_loss_ema:.6f}\"\n",
        "                    )\n",
        "\n",
        "        if (step + 1) % args.save_interval == 0:\n",
        "            if jax.process_index() == 0:\n",
        "                logging.info(f\"Saving checkpoint at step {step + 1}\")\n",
        "            opt_rngs, opt_state_no_rngs = nnx.filter_state(opt_state, nnx.RngKey, ...)\n",
        "            opt_rng_keys = jax.tree.map(jax.random.key_data, opt_rngs)\n",
        "\n",
        "            ema_rngs, ema_state_no_rngs = nnx.filter_state(ema_state, nnx.RngKey, ...)\n",
        "            ema_rng_keys = jax.tree.map(jax.random.key_data, ema_rngs)\n",
        "            if jax.process_index() == 0:\n",
        "                logging.info(f\"Opt rngs: {opt_rngs}\")\n",
        "                logging.info(f\"EMA rngs: {ema_rngs}\")\n",
        "                logging.info(f\"Opt state no rngs: {opt_state_no_rngs}\")\n",
        "                logging.info(f\"EMA state no rngs: {ema_state_no_rngs}\")\n",
        "            ckpt_mngr.save(\n",
        "                step + 1,\n",
        "                args=ocp.args.Composite(\n",
        "                    opt_state=ocp.args.StandardSave(opt_state_no_rngs),\n",
        "                    opt_rngs=ocp.args.StandardSave(opt_rng_keys),\n",
        "                    ema_state=ocp.args.StandardSave(ema_state_no_rngs),\n",
        "                    ema_rngs=ocp.args.StandardSave(ema_rng_keys),\n",
        "                ),\n",
        "            )\n",
        "            if jax.process_index() == 0:\n",
        "                logging.info(f\"Checkpoint saved successfully\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "x2ps9w3CSl7G",
        "outputId": "55f69b95-2f81-4756-a9e5-e58330bbb199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-20 22:51:24,071 - INFO - Starting training with args: Namespace(experiment_name='fsdp', gpu=False, steps=10000, test_interval=1000, batch_size=256, log_interval=100, save_interval=1000, checkpoint_dir='/content/checkpoints', output_dir='/content/outputs', lr=1e-05, add_noise=False)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "jax.distributed.initialize() must be called before any JAX calls that might initialise the XLA backend. This includes any computation, but also calls to jax.devices, jax.device_put, and others.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1240547175.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2645867000.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#     \"gs://\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# ), \"Checkpoint directory must be a GCS path\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Available JAX devices: {jax.devices()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/distributed.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(coordinator_address, num_processes, process_id, local_device_ids, cluster_detection_method, initialization_timeout, coordinator_bind_address)\u001b[0m\n\u001b[1;32m    257\u001b[0m   \"\"\"\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mxla_bridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends_are_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     raise RuntimeError(\"jax.distributed.initialize() must be called before \"\n\u001b[0m\u001b[1;32m    260\u001b[0m                         \u001b[0;34m\"any JAX calls that might initialise the XLA backend. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                         \"This includes any computation, but also calls to jax.devices, jax.device_put, and others.\")\n",
            "\u001b[0;31mRuntimeError\u001b[0m: jax.distributed.initialize() must be called before any JAX calls that might initialise the XLA backend. This includes any computation, but also calls to jax.devices, jax.device_put, and others."
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    experiment_name=\"fsdp\",\n",
        "    gpu=False,\n",
        "    steps=10_000,\n",
        "    test_interval=1000,\n",
        "    batch_size=256,\n",
        "    log_interval=100,\n",
        "    save_interval=1000,\n",
        "    checkpoint_dir=os.path.abspath(\"checkpoints/\"),\n",
        "    output_dir=os.path.abspath(\"outputs/\"),\n",
        "    lr=1e-5,\n",
        "    add_noise=False\n",
        ")\n",
        "\n",
        "main(args)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "1mr5W7P-Ybhq"
      ],
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}